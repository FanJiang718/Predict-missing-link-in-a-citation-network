{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim, logging\n",
    "import re\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from networkx.algorithms import approximation as approx\n",
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "############################################\n",
    "# first step: data preprocessing\n",
    "############################################\n",
    "\n",
    "#load train data et node_information\n",
    "node_info = pd.read_csv(\"node_information.csv\", names=[\"ID\", \"year\", \"title\", \"authors\", \"journal\", \"abstracts\"])\n",
    "data_train = np.loadtxt(\"training_set.txt\")\n",
    "data_test = np.loadtxt(\"testing_set.txt\")\n",
    "data,label= data_train[:, 0:-1], data_train[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "#some useful fonction\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\-]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def node_information_preprocess(node_info):\n",
    "    node_info[\"title_\"] = node_info[\"title\"].apply(lambda string: clean_str(string).split(\" \"))\n",
    "    node_info[\"title_remove_stw\"] = node_info[\"title\"].apply(lambda string:\n",
    "                                    clean_str(gensim.parsing.preprocessing.remove_stopwords(string)).split(\" \") if isinstance(string, str) else [])\n",
    "    node_info[\"authors_\"] = node_info[\"authors\"].apply(lambda string: string.split(\",\") if isinstance(string, str) else [])\n",
    "    node_info[\"journal_\"] = node_info[\"journal\"].apply(lambda string: [string]if isinstance(string, str) else [])\n",
    "    node_info[\"journal_divide\"] = node_info[\"journal\"].apply(lambda string: string.strip(\".\").split(\".\") if isinstance(string, str) else [])\n",
    "    node_info[\"abstracts_\"] = node_info[\"abstracts\"].apply(lambda string:clean_str(string).split() if isinstance(string, str) else [])\n",
    "    return node_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# second step: feature enginneering\n",
    "############################################\n",
    "\n",
    "\n",
    "def feature_engineering(data,node_info,graph,graph2,graph_auth,model_abst,model_title):\n",
    "\n",
    "    data[\"title1\"] = data.apply(lambda row: node_info.loc[row[\"pos1\"], \"title_\"], axis=1)\n",
    "    data[\"title2\"] = data.apply(lambda row: node_info.loc[row[\"pos2\"], \"title_\"], axis=1)\n",
    "\n",
    "    data[\"title_remove_stw1\"] = data.apply(lambda row: node_info.loc[row[\"pos1\"], \"title_remove_stw\"], axis=1)\n",
    "    data[\"title_remove_stw2\"] = data.apply(lambda row: node_info.loc[row[\"pos2\"], \"title_remove_stw\"], axis=1)\n",
    "\n",
    "    data[\"authors1\"] = data.apply(lambda row: node_info.loc[row[\"pos1\"], \"authors_\"], axis=1)\n",
    "    data[\"authors2\"] = data.apply(lambda row: node_info.loc[row[\"pos2\"], \"authors_\"], axis=1)\n",
    "\n",
    "    data[\"journal1\"] = data.apply(lambda row: node_info.loc[row[\"pos1\"], \"journal_\"], axis=1)\n",
    "    data[\"journal2\"] = data.apply(lambda row: node_info.loc[row[\"pos2\"], \"journal_\"], axis=1)\n",
    "\n",
    "    data[\"journal_divide1\"] = data.apply(lambda row: node_info.loc[row[\"pos1\"], \"journal_divide\"], axis=1)\n",
    "    data[\"journal_divide2\"] = data.apply(lambda row: node_info.loc[row[\"pos2\"], \"journal_divide\"], axis=1)\n",
    "\n",
    "    ##############################################\n",
    "    ### Attribute Features\n",
    "    ##############################################\n",
    "\n",
    "    ### find years of the papers of and the difference between papers ,\n",
    "    # the paper cite another much possible as they are cloed to each other\n",
    "\n",
    "    data[\"year1\"] = data.apply(lambda row: node_info.loc[row[\"pos1\"], \"year\"], axis=1)\n",
    "    data[\"year2\"] = data.apply(lambda row: node_info.loc[row[\"pos2\"], \"year\"], axis=1)\n",
    "    data[\"diff_year\"] = np.abs(data[\"year1\"] - data[\"year2\"])\n",
    "\n",
    "    ### The number of commom word in the papers, it's much possible that the two papers have the bigger common word.\n",
    "\n",
    "    data[\"title_overlap\"] = data.apply(\n",
    "        lambda row: len(set(row[\"title_remove_stw1\"]).intersection(set(row[\"title_remove_stw2\"]))),axis=1)\n",
    "\n",
    "    ### The number of commom authors. MacRoberts and MacRoberts (1989) pointed out that self-citations are often observed\n",
    "    # because researchers are more familiar with their own research than the research of others.\n",
    "\n",
    "    data[\"author_overlap\"] = data.apply(\n",
    "        lambda row: len(set(row[\"authors1\"]).intersection(set(row[\"authors2\"]))), axis=1)\n",
    "\n",
    "    ###Is_self_citation the same as before\n",
    "\n",
    "    def is_self_cite(row):\n",
    "        self_cite = 0\n",
    "        for author in row[\"authors1\"]:\n",
    "            if author in row[\"authors2\"]:\n",
    "                self_cite = 1\n",
    "                break\n",
    "        return self_cite\n",
    "\n",
    "    data[\"is_self_cite\"] = data.apply(is_self_cite, axis=1)\n",
    "\n",
    "    ### The number of commom word in the journal. The journal contain the information of the paper\n",
    "\n",
    "    data[\"journal_overlap\"] = data.apply(\n",
    "        lambda row: len(set(row[\"journal_divide1\"]).intersection(set(row[\"journal_divide2\"]))), axis=1)\n",
    "\n",
    "    ### Is published in same journal As researchers tend to work in specific scientific communities,\n",
    "    #  two papers published in the same journal are likely to be connected.\n",
    "\n",
    "    data[\"journal_common\"] = data.apply(\n",
    "        lambda row: len(set(row[\"journal1\"]).intersection(set(row[\"journal2\"]))), axis=1)\n",
    "\n",
    "    ##############################################\n",
    "    ### Topological Features\n",
    "    ##############################################\n",
    "\n",
    "    ##################### For citation network #######################\n",
    "\n",
    "    ### The number of the neighbors for a paper,which is related that how populair a paper is\n",
    "    data[\"neighbors_1\"] = data.apply(lambda row: graph.degree(row[\"pos1\"]), axis=1)\n",
    "    data[\"neighbors_2\"] = data.apply(lambda row: graph.degree(row[\"pos2\"]), axis=1)\n",
    "\n",
    "    ### The number of common neighbours, nodes are highly clustered locally (Watts & Strogatz, 1998),\n",
    "    # which means two papers with more common neighbours tend to be connected.\n",
    "    data[\"common_neighbors\"] = data.apply(lambda row: len(list(nx.common_neighbors(graph, row[\"pos1\"], row[\"pos2\"]))),\n",
    "                                          axis=1)\n",
    "\n",
    "    ### Link-based Jaccard coefficient, The link-based Jaccard coefficient represents\n",
    "    # the relative value of the number of common neighbours.\n",
    "\n",
    "    def Jaccard_coefficient(row):\n",
    "        union = len(set(graph[row[\"pos1\"]]) | set(graph[row[\"pos2\"]]))\n",
    "        if union == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return row[\"common_neighbors\"] / union\n",
    "\n",
    "    data[\"jaccard\"] = data.apply(Jaccard_coefficient, axis=1)\n",
    "\n",
    "    def Cosine(row):\n",
    "        union = graph.degree(row[\"pos1\"]) * graph.degree(row[\"pos2\"])\n",
    "        if union == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return row[\"common_neighbors\"] / union\n",
    "\n",
    "    data[\"cosine\"] = data.apply(Cosine, axis=1)\n",
    "\n",
    "    ### adamic_adar_index.Weights connections with rare nodes more heavily.based on the concept that common elements with very large neighbourhoods\n",
    "    # are lesser significant when predicting a connection between two nodes compared with elements shared between a small number of nodes.\n",
    "\n",
    "\n",
    "    def adamic_adar_index(graph, u, v):\n",
    "        return sum(1 / np.log(graph.degree(w)) for w in nx.common_neighbors(graph, u, v))\n",
    "\n",
    "    data[\"adamic_adar_index\"] = data.apply(lambda row: adamic_adar_index(graph, row[\"pos1\"], row[\"pos2\"]), axis=1)\n",
    "\n",
    "    data[\"resource_allocation\"] = data.apply(\n",
    "        lambda row: sum(1. / graph.degree(w) for w in nx.common_neighbors(graph, row[\"pos1\"], row[\"pos2\"])), axis=1)\n",
    "\n",
    "    ### preferential_attachement : \"cumulative advantage\",that the probability of co-authorship of x and y\n",
    "    # is correlated with the product of the number of collaborators of x and y.\n",
    "\n",
    "    data[\"preferential_attachement\"] = data.apply(lambda row: graph.degree(row[\"pos1\"]) * graph.degree(row[\"pos2\"]),\n",
    "                                                  axis=1)\n",
    "\n",
    "    ###same_community that two papers are more likely to be connected if they are in the same clusters.\n",
    "\n",
    "    partition = community.best_partition(graph)\n",
    "    #data[\"community1\"] = data.apply(lambda row: partition[row[\"pos1\"]], axis=1)\n",
    "    #data[\"community2\"] = data.apply(lambda row: partition[row[\"pos2\"]], axis=1)\n",
    "    data[\"same_community\"] = data.apply(lambda row: int(partition[row[\"pos1\"]] == partition[row[\"pos2\"]]), axis=1)\n",
    "    # X_pd[\"connectivity\"] = X_pd.apply(lambda row: approx.local_node_connectivity(graph, row[\"pos1\"], row[\"pos2\"]), axis=1)\n",
    "\n",
    "    ###betweenness_centrality measuring the influence a node has over the spread of information through the network.\n",
    "\n",
    "    btw = nx.betweenness_centrality(graph, 1000)\n",
    "    data[\"betweeness_diff\"] = data.apply(lambda row: btw[row[\"pos2\"]] - btw[row[\"pos1\"]], axis=1)\n",
    "\n",
    "\n",
    "    ### This value represents the attracting force between from and to,\n",
    "\n",
    "    data[\"in_link_diff\"] = data.apply(lambda row: len(graph2.in_edges(row[\"pos2\"])) - len(graph2.in_edges(row[\"pos1\"])),\n",
    "                                      axis=1)\n",
    "\n",
    "    ### Although we are analyzing the citation model, it is the authors who make the decision of citations.\n",
    "    # Therefore, we think it is also useful to incorporate the authors’ properties in our citation model.\n",
    "    # the maximum number of collaborations happened between each author of pa- per A and each author of paper B\n",
    "\n",
    "    def max_author_collaboration(row):\n",
    "        max_author_collaboration = 0\n",
    "        if len(row[\"authors1\"]) * len(row[\"authors2\"]) == 0:\n",
    "            return 0\n",
    "        for author1 in row[\"authors1\"]:\n",
    "            for author2 in row[\"authors2\"]:\n",
    "                if graph_auth.has_edge(author1, author2):\n",
    "                    max_author_collaboration = max(max_author_collaboration,graph_auth[author1][author2]['weight'])\n",
    "        return max_author_collaboration\n",
    "\n",
    "    data[\"max_author_collaboration\"] = data.apply(max_author_collaboration, axis=1)\n",
    "\n",
    "    # the average number of collaboration between authors of paper A and authors of paper B;\n",
    "    def mean_author_collaboration(row):\n",
    "        mean_author_collaboration = 0\n",
    "\n",
    "        if len(row[\"authors1\"]) * len(row[\"authors2\"]) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            for author1 in row[\"authors1\"]:\n",
    "                for author2 in row[\"authors2\"]:\n",
    "                    if graph_auth.has_edge(author1, author2):\n",
    "                        mean_author_collaboration += graph_auth[author1][author2]['weight']\n",
    "\n",
    "        return mean_author_collaboration /(len(row[\"authors1\"]) * len(row[\"authors2\"]))\n",
    "    data[\"mean_author_collaboration\"] = data.apply(mean_author_collaboration, axis=1)\n",
    "\n",
    "    ###########################################\n",
    "    #Semantic Features\n",
    "    ###########################################\n",
    "\n",
    "    ### text similarity: the semantic similarity between two paper titles defined by “Word2vec”.\n",
    "    # Papers with similar content tend to cite each other.\n",
    "\n",
    "    # Titles similarity\n",
    "\n",
    "    docvecs_title = model_title.docvecs\n",
    "    data[\"simi_title\"] = data.apply(lambda row: docvecs_title.similarity(int(row[\"pos1\"]), int(row[\"pos2\"])), axis=1)\n",
    "\n",
    "    # abstracts similarity\n",
    "\n",
    "    docvecs_abst = model_abst.docvecs\n",
    "    data[\"simi_abst\"] = data.apply(lambda row: docvecs_abst.similarity(int(row[\"pos1\"]), int(row[\"pos2\"])), axis=1)\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_creation(data,label,data_test,node_info,validation = False):\n",
    "\n",
    "\n",
    "    node_info = node_information_preprocess(node_info)\n",
    "\n",
    "    ### Creation of features\n",
    "    data = pd.DataFrame(data.astype(int), columns=[\"ID1\", \"ID2\"])\n",
    "    data_test = pd.DataFrame(data_test.astype(int), columns=[\"ID1\", \"ID2\"])\n",
    "\n",
    "    ### find the position of the papers in node_information\n",
    "    data[\"pos1\"] = data.apply(lambda row: node_info.loc[row[\"ID1\"] == node_info.ID].index[0], axis=1)\n",
    "    data[\"pos2\"] = data.apply(lambda row: node_info.loc[row[\"ID2\"] == node_info.ID].index[0], axis=1)\n",
    "\n",
    "    data_test[\"pos1\"] = data_test.apply(lambda row: node_info.loc[row[\"ID1\"] == node_info.ID].index[0], axis=1)\n",
    "    data_test[\"pos2\"] = data_test.apply(lambda row: node_info.loc[row[\"ID2\"] == node_info.ID].index[0], axis=1)\n",
    "\n",
    "    if validation:\n",
    "        data, data_val, label, label_val = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "    graph = nx.Graph()\n",
    "    graph2 = nx.DiGraph()\n",
    "    for i in range(len(node_info)):\n",
    "        graph.add_node(i)\n",
    "        graph2.add_node(i)\n",
    "    for i in range(len(label)):\n",
    "        if label[i] == 1:\n",
    "            graph.add_edge(data.loc[i, \"pos1\"], data.loc[i, \"pos2\"])\n",
    "            graph2.add_edge(data.loc[i, \"pos1\"], data.loc[i, \"pos2\"])\n",
    "\n",
    "\n",
    "    graph_auth = nx.Graph()\n",
    "    for authors in node_info[\"authors_\"]:\n",
    "        for i in range(len(authors)):\n",
    "            for j in range(i):\n",
    "                if graph_auth.has_edge(authors[i],authors[j]):\n",
    "                    graph_auth.add_edge(authors[i], authors[j],weight = graph_auth[authors[i]][authors[j]]['weight']+1)\n",
    "                else:\n",
    "                    graph_auth.add_edge(authors[i], authors[j],weight=1)\n",
    "\n",
    "    ### build a word vector model for abstract\n",
    "    abstracts = []\n",
    "    for i, t in enumerate(node_info[\"abstracts\"]):\n",
    "        arr = gensim.parsing.preprocess_string(t)\n",
    "        abstracts.append(gensim.models.doc2vec.TaggedDocument(arr, [i]))\n",
    "\n",
    "    model_abst = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=5, epochs=100)\n",
    "    model_abst.build_vocab(abstracts)\n",
    "    model_abst.train(abstracts, total_examples=model_abst.corpus_count, epochs=model_abst.epochs)\n",
    "    #docvecs_abst.save(\"./docvec_abstract\")\n",
    "\n",
    "    titles = []\n",
    "    for i, t in enumerate(node_info[\"title\"]):\n",
    "        arr = gensim.parsing.preprocess_string(t)\n",
    "        titles.append(gensim.models.doc2vec.TaggedDocument(arr, [i]))\n",
    "    model_title = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=5, epochs=100)\n",
    "    model_title.build_vocab(titles)\n",
    "    model_title.train(titles, total_examples=model_title.corpus_count, epochs=model_title.epochs)\n",
    "    # docvecs_abst.save(\"./docvec_title\")\n",
    "\n",
    "    data = feature_engineering(data, node_info, graph, graph2, graph_auth, model_abst, model_title)\n",
    "    data_test = feature_engineering(data_test, node_info, graph, graph2, graph_auth, model_abst,\n",
    "                                    model_title)\n",
    "    if validation:\n",
    "        data_val = feature_engineering(data_val, node_info, graph, graph2, graph_auth, model_abst, model_title)\n",
    "        return data,label,data_val,label_val,data_test\n",
    "\n",
    "    else:\n",
    "        return data,label,data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_,label_,data_test = feature_creation(data[:100,:],label[:100],data_test[:100],node_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_,label_,data_val,label_val,data_test = feature_creation(data[:100,:],label[:100],data_test[:100],node_info,validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# third step: model selection\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
